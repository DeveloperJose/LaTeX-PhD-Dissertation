Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Perez2022,
abstract = {Physics Informed Neural Networks (PINNs) incorporate known physics equations into a network to reduce training time and increase accuracy. Traditional PINNs approaches are based on dense networks that do not consider the fact that simulations are a type of sequential data. Long-Short Term Memory (LSTM) networks are a modified version of Recurrent Neural Networks (RNNs) which are used to analyze sequential datasets. We propose a Physics Informed LSTM network that leverages the power of LSTMs for sequential datasets that also incorporates the governing physics equations of 2D incompressible Navier-Stokes fluid to analyze fluid flow around a stationary geometry resembling the water braking mechanism at the Holloman High-Speed Test Track. Currently, simulation data to analyze the fluid flow of the braking mechanism is generated through ANSYS and is costly, taking several days to generate a single simulation. By incorporating physics equations, our proposed Physics-Informed LSTM network was able to predict the last 20{\%} of a simulation given the first 80{\%} within a small margin of error in a shorter amount of time than a non-informed LSTM. This demonstrates the potential that physics-informed networks that leverage sequential information may have at speeding up computational fluid dynamics simulations and serves as a first step towards adapting PINNs for more advanced network architectures.},
author = {P{\'{e}}rez, Jos{\'{e}} and Baez, Rafael and Terrazas, Jose and Rodr{\'{i}}guez, Arturo and Villanueva, Daniel and Fuentes, Olac and Kumar, Vinod and Paez, Brandon and Cruz, Abdiel},
booktitle = {American Society of Mechanical Engineers, Fluids Engineering Division (Publication) FEDSM},
doi = {10.1115/FEDSM2022-86953},
isbn = {9780791885840},
issn = {08888116},
keywords = {HHSTT,LSTM,PINNs},
publisher = {American Society of Mechanical Engineers Digital Collection},
title = {{Physics-Informed Long-Short Term Memory Neural Network Performance on Holloman High-Speed Test Track Sled Study}},
url = {https://dx.doi.org/10.1115/FEDSM2022-86953},
volume = {2},
year = {2022}
}
@inproceedings{Zhao2019,
abstract = {Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images. We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation.},
archivePrefix = {arXiv},
arxivId = {1902.09383},
author = {Zhao, Amy and Balakrishnan, Guha and Durand, Fredo and Guttag, John V. and Dalca, Adrian V.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00874},
eprint = {1902.09383},
isbn = {9781728132938},
issn = {10636919},
keywords = {Bibtex,Biological and Cell Microscopy,Image and Video Synthesis,Medical},
mendeley-tags = {Bibtex},
month = {jun},
pages = {8535--8545},
title = {{Data augmentation using learned transformations for one-shot medical image segmentation}},
volume = {2019-June},
year = {2019}
}
@article{FCN,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
issn = {01628828},
journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {3431--3440},
pmid = {27244717},
title = {{Fully convolutional networks for semantic segmentation}},
url = {https://api.semanticscholar.org/CorpusID:1629541},
volume = {39},
year = {2014}
}
@inproceedings{GPT3,
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
archivePrefix = {arXiv},
arxivId = {2005.14165},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {2005.14165},
issn = {10495258},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
title = {{Language models are few-shot learners}},
url = {https://api.semanticscholar.org/CorpusID:218971783},
volume = {2020-Decem},
year = {2020}
}
@inproceedings{GRU,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
pages = {1724--1734},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
url = {https://api.semanticscholar.org/CorpusID:5590763},
year = {2014}
}
@article{UnifiedLoss,
archivePrefix = {arXiv},
arxivId = {2102.04525},
author = {Yeung, Michael and Sala, Evis and Sch{\"{o}}nlieb, Carola Bibiane and Rundo, Leonardo},
doi = {10.1016/J.COMPMEDIMAG.2021.102026},
eprint = {2102.04525},
issn = {0895-6111},
journal = {Computerized Medical Imaging and Graphics},
keywords = { Convolutional neural networks, Loss function, Machine learning, Medical image segmentation,Class imbalance},
pages = {102026},
pmid = {34953431},
publisher = {Pergamon},
title = {{Unified Focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation}},
volume = {95},
year = {2022}
}
@article{MNIST,
abstract = {In this issue, Best of the Web presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research. {\textcopyright} 2012 IEEE.},
author = {Deng, Li},
doi = {10.1109/MSP.2012.2211477},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {141--142},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The MNIST database of handwritten digit images for machine learning research}},
volume = {29},
year = {2012}
}
@inproceedings{PrototypicalNetworks,
author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S},
booktitle = {Neural Information Processing Systems},
title = {{Prototypical Networks for Few-shot Learning}},
url = {https://api.semanticscholar.org/CorpusID:309759},
year = {2017}
}
@article{MLPMixer,
author = {Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
journal = {ArXiv},
title = {{MLP-Mixer: An all-MLP Architecture for Vision}},
url = {https://api.semanticscholar.org/CorpusID:233714958},
volume = {abs/2105.01601},
year = {2021}
}
@article{SelfDriving1,
author = {Mao, Jiageng and Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
journal = {International Journal of Computer Vision},
pages = {1909--1963},
title = {{3D Object Detection for Autonomous Driving: A Comprehensive Survey}},
url = {https://api.semanticscholar.org/CorpusID:257921140},
volume = {131},
year = {2022}
}
@inproceedings{Siamese1,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{\%} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Koch, Gregory R and van der Spoel, Evie and Rozing, Maarten P. and Houwing-Duistermaat, Jeanine J. and {Eline Slagboom}, P. and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
booktitle = {ICML - Deep Learning Workshop},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {19454589},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Siamese Neural Networks for One-Shot Image Recognition}},
url = {https://api.semanticscholar.org/CorpusID:13874643},
volume = {7},
year = {2015}
}
@article{LSTM,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
journal = {Neural Computation},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {https://api.semanticscholar.org/CorpusID:1915014},
volume = {9},
year = {1997}
}
@inproceedings{Bibek2023,
abstract = {Large-scale study of glaciers improves our understanding of global glacier change and is imperative for monitoring the ecological environment, preventing disasters, and studying the effects of global climate change. Glaciers in the Hindu Kush Himalaya (HKH) are particularly interesting as the HKH is one of the world's most sensitive regions for climate change. In this work, we: (1) propose a modified version of the U-Net for large-scale, spatially non-overlapping, clean glacial ice, and debris-covered glacial ice segmentation; (2) introduce a novel self-learning boundary-aware loss to improve debris-covered glacial ice segmentation performance; and (3) propose a feature-wise saliency score to understand the contribution of each feature in the multispectral Landsat 7 imagery for glacier mapping. Our results show that the debris-covered glacial ice segmentation model trained using self-learning boundary-aware loss outperformed the model trained using dice loss. Furthermore, we conclude that red, shortwave infrared, and near-infrared bands have the highest contribution toward debris-covered glacial ice segmentation from Landsat 7 images.},
author = {Aryal, Bibek and Miles, Katie E. and {Vargas Zesati}, Sergio A. and Fuentes, Olac},
booktitle = {Proceedings of the Northern Lights Deep Learning Workshop},
doi = {10.7557/18.6789},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
month = {may},
title = {{Boundary Aware U-Net for Glacier Segmentation}},
volume = {4},
year = {2023}
}
@misc{GlacierImportance,
author = {{National Snow and Ice Data Center}},
title = {{Why Glaciers Matter | National Snow and Ice Data Center}},
url = {https://nsidc.org/learn/parts-cryosphere/glaciers/why-glaciers-matter},
urldate = {2024-02-29},
year = {2023}
}
@article{StyleGAN1,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948},
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
doi = {10.1109/TPAMI.2020.2970919},
eprint = {1812.04948},
issn = {19393539},
journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Bibtex,Generative models,deep learning,neural networks},
mendeley-tags = {Bibtex},
number = {12},
pages = {4396--4405},
pmid = {32012000},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
url = {https://api.semanticscholar.org/CorpusID:54482423},
volume = {43},
year = {2018}
}
@article{EfficientNet,
author = {Tan, Mingxing and Le, Quoc V},
journal = {ArXiv},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {https://api.semanticscholar.org/CorpusID:167217261},
volume = {abs/1905.11946},
year = {2019}
}
@article{StyleGAN2,
abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
archivePrefix = {arXiv},
arxivId = {1912.04958},
author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
doi = {10.1109/CVPR42600.2020.00813},
eprint = {1912.04958},
issn = {10636919},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {8107--8116},
title = {{Analyzing and Improving the Image Quality of StyleGAN}},
url = {https://api.semanticscholar.org/CorpusID:209202273},
year = {2019}
}
@inproceedings{ImageNet,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li Jia and Li, Kai and Fei-Fei, Li},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2009},
doi = {10.1109/CVPR.2009.5206848},
isbn = {9781424439911},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {248--255},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
url = {https://api.semanticscholar.org/CorpusID:57246310},
year = {2009}
}
@article{ReLU,
abstract = {A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattern is presented, and it responds to one particular feature of the input pattern, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments. Copyright {\textcopyright} 1970 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Fukushima},
doi = {10.1109/TSSC.1969.300225},
issn = {21682887},
journal = {IEEE Transactions on Systems Science and Cybernetics},
number = {4},
pages = {322--333},
title = {{Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements}},
url = {https://api.semanticscholar.org/CorpusID:206799280},
volume = {5},
year = {1969}
}
@article{VIT,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
journal = {ArXiv},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {https://api.semanticscholar.org/CorpusID:225039882},
volume = {abs/2010.1},
year = {2020}
}
@inproceedings{Baik1,
author = {Baik, Sungyong and Choi, Janghoon and Kim, Heewon and Cho, Dohee and Min, Jaesik and Lee, Kyoung Mu},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
pages = {9465--9474},
title = {{Meta-Learning With Task-Adaptive Loss Function for Few-Shot Learning}},
year = {2021}
}
@phdthesis{Godwill1,
author = {Amankwa, Godwill},
keywords = {Bibtex,Computer science; Astrophysics; Astronomy; Artific},
mendeley-tags = {Bibtex},
month = {jan},
school = {University of Texas, El Paso},
title = {{Modeling the Spatiotemporal Dynamics of Active Regions on the Sun Using Deep Neural Networks}},
year = {2021}
}
@article{StableDiffusion1,
abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
archivePrefix = {arXiv},
arxivId = {2112.10752},
author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"{o}}rn Bjorn},
doi = {10.1109/CVPR52688.2022.01042},
eprint = {2112.10752},
isbn = {9781665469463},
issn = {10636919},
journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Bibtex,Image and video synthesis and generation},
mendeley-tags = {Bibtex},
pages = {10674--10685},
title = {{High-Resolution Image Synthesis with Latent Diffusion Models}},
url = {https://api.semanticscholar.org/CorpusID:245335280},
volume = {2022-June},
year = {2021}
}
@inproceedings{Attention1,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U Von and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {{Attention is All you Need}},
url = {https://proceedings.neurips.cc/paper{\_}files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
volume = {30},
year = {2017}
}
@article{RelationNet,
author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H S and Hospedales, Timothy M},
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages = {1199--1208},
title = {{Learning to Compare: Relation Network for Few-Shot Learning}},
url = {https://api.semanticscholar.org/CorpusID:4412459},
year = {2017}
}
@article{YOLOv1,
author = {Redmon, Joseph and Divvala, Santosh Kumar and Girshick, Ross B and Farhadi, Ali},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {779--788},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {https://api.semanticscholar.org/CorpusID:206594738},
year = {2015}
}
@inproceedings{TADAM,
abstract = {Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14{\%} in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.},
archivePrefix = {arXiv},
arxivId = {1805.10123},
author = {Oreshkin, Boris N. and Rodriguez, Pau and Lacoste, Alexandre},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1805.10123},
issn = {10495258},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {721--731},
title = {{Tadam: Task dependent adaptive metric for improved few-shot learning}},
url = {https://api.semanticscholar.org/CorpusID:44061218},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{MAML,
author = {Finn, Chelsea and Abbeel, P and Levine, Sergey},
booktitle = {International Conference on Machine Learning},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {https://api.semanticscholar.org/CorpusID:6719686},
year = {2017}
}
@article{UNet,
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
journal = {ArXiv},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {https://api.semanticscholar.org/CorpusID:3719281},
volume = {abs/1505.0},
year = {2015}
}
@article{PINNS,
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
doi = {10.1016/J.JCP.2018.10.045},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Bibtex,Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods},
mendeley-tags = {Bibtex},
month = {feb},
pages = {686--707},
publisher = {Academic Press},
title = {{Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations}},
url = {https://api.semanticscholar.org/CorpusID:57379996},
volume = {378},
year = {2019}
}
@inproceedings{BERT,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
eprint = {1810.04805},
isbn = {9781950737130},
keywords = {Bibtex},
mendeley-tags = {Bibtex},
pages = {4171--4186},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
url = {https://api.semanticscholar.org/CorpusID:52967399},
volume = {1},
year = {2019}
}
@inproceedings{COCO,
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
booktitle = {European Conference on Computer Vision},
title = {{Microsoft COCO: Common Objects in Context}},
url = {https://api.semanticscholar.org/CorpusID:14113767},
year = {2014}
}
@inproceedings{SupervisedContrastiveLearning,
author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Larochelle, H and Ranzato, M and Hadsell, R and Balcan, M F and Lin, H},
pages = {18661--18673},
publisher = {Curran Associates, Inc.},
title = {{Supervised Contrastive Learning}},
url = {https://proceedings.neurips.cc/paper{\_}files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
volume = {33},
year = {2020}
}
@article{GoLIVE1,
abstract = {We report on the maturation of optical satellite-image-based ice velocity mapping over the ice sheets and large glacierized areas, enabled by the high radiometric resolution and internal geometric accuracy of Landsat 8's Operational Land Imager (OLI). Detailed large-area single-season mosaics and time-series maps of ice flow were created using data spanning June 2013 to June 2015. The 12-bit radiometric quantization and 15-m pixel scale resolution of OLI band 8 enable displacement tracking of subtle snow-drift patterns on ice sheet surfaces at {\~{}} 1 m precision. Ice sheet and snowfield snow-drift features persist for typically 16 to 64 days, and up to 432 days, depending primarily on snow accumulation rates. This results in spatially continuous mapping of ice flow, extending the mapping capability beyond crevassed areas. Our method uses image chip cross-correlation and sub-pixel peak-fitting in matching Landsat path/row pairs. High-pass filtering is applied to the imagery to enhance local surface texture. The current high image acquisition rates of Landsat 8 (725 scenes per day globally) reduces the impact of high cloudiness in polar and mountain terrain and allows rapid compilation of large areas, or dense temporal coverage of seasonal ice flow variations. The results rival the coverage and accuracy of interferometric Synthetic Aperture Radar (InSAR) mapping.},
annote = {Landsat 8 Science Results},
author = {Fahnestock, Mark and Scambos, Ted and Moon, Twila and Gardner, Alex and Haran, Terry and Klinger, Marin},
doi = {10.1016/j.rse.2015.11.023},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Antarctica,Glaciers,Greenland,Ice flow,Landsat,Remote sensing},
pages = {84--94},
title = {{Rapid large-area mapping of ice flow using Landsat 8}},
url = {https://www.sciencedirect.com/science/article/pii/S003442571530211X},
volume = {185},
year = {2016}
}
@misc{ANSYS,
address = {Canonsburg, PA},
publisher = {ANSYS, Inc.},
title = {{ANSYS Fluent}}
}
@article{MANet,
abstract = {Semantic segmentation of remote sensing images plays an important role in a wide range of applications, including land resource management, biosphere monitoring, and urban planning. Although the accuracy of semantic segmentation in remote sensing images has been increased significantly by deep convolutional neural networks, several limitations exist in standard models. First, for encoder-decoder architectures such as U-Net, the utilization of multiscale features causes the underuse of information, where low-level features and high-level features are concatenated directly without any refinement. Second, long-range dependencies of feature maps are insufficiently explored, resulting in suboptimal feature representations associated with each semantic class. Third, even though the dot-product attention mechanism has been introduced and utilized in semantic segmentation to model long-range dependencies, the large time and space demands of attention impede the actual usage of attention in application scenarios with large-scale input. This article proposed a multiattention network (MANet) to address these issues by extracting contextual dependencies through multiple efficient attention modules. A novel attention mechanism of kernel attention with linear complexity is proposed to alleviate the large computational demand in attention. Based on kernel attention and channel attention, we integrate local feature maps extracted by ResNet-50 with their corresponding global dependencies and reweight interdependent channel maps adaptively. Numerical experiments on two large-scale fine-resolution remote sensing datasets demonstrate the superior performance of the proposed MANet. Code is available at https://github.com/lironui/Multi-Attention-Network.},
author = {Li, Rui and Zheng, Shunyi and Zhang, Ce and Duan, Chenxi and Su, Jianlin and Wang, Libo and Atkinson, Peter M.},
doi = {10.1109/TGRS.2021.3093977},
issn = {15580644},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Attention mechanism,fine-resolution remote sensing images,semantic segmentation},
pages = {1--13},
title = {{Multiattention Network for Semantic Segmentation of Fine-Resolution Remote Sensing Images}},
url = {https://api.semanticscholar.org/CorpusID:221507549},
volume = {60},
year = {2020}
}
@inproceedings{CNN1,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Advances In Neural Information Processing Systems},
doi = {10.1061/(ASCE)GT.1943-5606.0001284},
eprint = {1102.0183},
isbn = {9780415468442},
issn = {1090-0241},
number = {25},
pages = {1097----1105},
pmid = {15287819},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
